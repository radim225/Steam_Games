{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steam Review Summarization - Optimized\n",
    "\n",
    "**Model:** distilbart-cnn-12-6 (download via terminal first)\n",
    "\n",
    "```bash\n",
    "# Mac/Linux\n",
    "pip install -U \"huggingface_hub[cli]\" hf_transfer\n",
    "export HF_HUB_ENABLE_HF_TRANSFER=1\n",
    "huggingface-cli download sshleifer/distilbart-cnn-12-6 --local-dir ~/hf_cache/distilbart\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration - EDIT THESE SETTINGS\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# START FROM SPECIFIC APP_ID (set to None to start from beginning)\n",
    "START_APP_ID = None  # Example: 1000 to start from app_id 1000\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = '../data'\n",
    "PROCESSED_DIR = os.path.join(DATA_DIR, 'processed')\n",
    "MODEL_PATH = '/Users/radimsoukal/hf_cache/distilbart'  # Adjust for Windows\n",
    "\n",
    "# Processing settings\n",
    "NUM_WORKERS = 7  # Adjust based on CPU cores\n",
    "CHECKPOINT_EVERY = 100\n",
    "\n",
    "# Files to use/create\n",
    "COMBINED_REVIEWS_FILE = os.path.join(PROCESSED_DIR, 'combined_reviews_cache.pkl')\n",
    "FINAL_OUTPUT_FILE = os.path.join(PROCESSED_DIR, 'review_summaries_COMPLETE.csv')\n",
    "\n",
    "print(\"✓ Configuration loaded\")\n",
    "print(f\"  Start from app_id: {START_APP_ID if START_APP_ID else 'beginning'}\")\n",
    "print(f\"  Model path: {MODEL_PATH}\")\n",
    "print(f\"  Workers: {NUM_WORKERS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load or create combined reviews (efficient - uses cache)\n",
    "import pickle\n",
    "import glob\n",
    "import re\n",
    "import string\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "os.makedirs(PROCESSED_DIR, exist_ok=True)\n",
    "\n",
    "# Try to load from cache first\n",
    "if os.path.exists(COMBINED_REVIEWS_FILE):\n",
    "    print(\"Loading combined reviews from cache...\")\n",
    "    combined_df = pd.read_pickle(COMBINED_REVIEWS_FILE)\n",
    "    print(f\"✓ Loaded {len(combined_df)} games from cache\")\n",
    "else:\n",
    "    print(\"Creating combined reviews (first time)...\")\n",
    "    \n",
    "    # Load raw CSV files\n",
    "    csv_files = sorted(glob.glob(os.path.join(DATA_DIR, 'raw', 'app_reviews_*.csv')))\n",
    "    print(f\"Found {len(csv_files)} CSV files\")\n",
    "    \n",
    "    dfs = [pd.read_csv(f) for f in csv_files]\n",
    "    combined_df = pd.concat(dfs, ignore_index=True)\n",
    "    print(f\"Combined shape: {combined_df.shape}\")\n",
    "    \n",
    "    # Clean and combine reviews\n",
    "    def clean_text(text):\n",
    "        if not isinstance(text, str) or not text.strip():\n",
    "            return None\n",
    "        text = re.sub(r'<[^>]+>', '', text)\n",
    "        text = re.sub(r'http[s]?://\\S+|www\\.\\S+', '', text)\n",
    "        text = re.sub(r'[\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\\U0001F680-\\U0001F6FF\\U0001F1E0-\\U0001F1FF]+', '', text)\n",
    "        text = text.replace('\\r', ' ').replace('\\n', ' ').replace('\\t', ' ')\n",
    "        allowed_chars = set(string.ascii_letters + string.digits + ' .,!?\\'-')\n",
    "        text = ''.join(char for char in text if char in allowed_chars)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return text if text else None\n",
    "\n",
    "    def combine_reviews(row, review_cols):\n",
    "        reviews = []\n",
    "        for col in review_cols:\n",
    "            review = row.get(col)\n",
    "            if review is not None and review != '':\n",
    "                cleaned = clean_text(review)\n",
    "                if cleaned:\n",
    "                    reviews.append(cleaned)\n",
    "        unique_reviews = list(dict.fromkeys(reviews))  # Remove duplicates, preserve order\n",
    "        return ' [SEP] '.join(unique_reviews) if unique_reviews else ''\n",
    "\n",
    "    review_columns = [f'review_{i}' for i in range(1, 101)]\n",
    "    combined_df['combined_reviews'] = combined_df.apply(\n",
    "        lambda row: combine_reviews(row, review_columns), axis=1\n",
    "    )\n",
    "    \n",
    "    # Save to cache\n",
    "    combined_df.to_pickle(COMBINED_REVIEWS_FILE)\n",
    "    print(f\"✓ Created and cached {len(combined_df)} combined reviews\")\n",
    "\n",
    "print(f\"Non-empty reviews: {(combined_df['combined_reviews'] != '').sum()}\")\n",
    "print(f\"Avg length: {combined_df['combined_reviews'].str.len().mean():.0f} chars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load summarization model from local cache\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "\n",
    "print(f\"Loading model from: {MODEL_PATH}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, local_files_only=True)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_PATH, local_files_only=True)\n",
    "summarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer, device=-1)\n",
    "\n",
    "print(\"✓ Model loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warm-up model (first inference is slow)\n",
    "import time\n",
    "\n",
    "print(\"Testing model (warm-up, takes 2-5 min first time)...\")\n",
    "test_text = \"This game features engaging gameplay, beautiful graphics, and challenging difficulty.\"\n",
    "start = time.time()\n",
    "test_result = summarizer(test_text, max_length=50, min_length=10, do_sample=False)\n",
    "elapsed = time.time() - start\n",
    "\n",
    "print(f\"✓ Test complete in {elapsed:.1f}s\")\n",
    "print(f\"Output: {test_result[0]['summary_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarization functions with chunking and guidance\n",
    "\n",
    "GUIDANCE_PROMPT = \"\"\"Summarize the following user reviews into a concise, neutral third-person description of the game. Focus on: genre, gameplay mechanics, core features or modes, campaign or multiplayer aspects, and overall atmosphere or difficulty. Avoid opinions, memes, and repetition.\n",
    "\n",
    "Reviews:\"\"\"\n",
    "\n",
    "def split_into_chunks(text, max_chunk_size=3500, overlap=200):\n",
    "    if not text or len(text) <= max_chunk_size:\n",
    "        return [text] if text else []\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + max_chunk_size\n",
    "        if end < len(text):\n",
    "            for sep in ['. ', '! ', '? ', ' [SEP] ']:\n",
    "                last_sep = text[start:end].rfind(sep)\n",
    "                if last_sep != -1:\n",
    "                    end = start + last_sep + len(sep)\n",
    "                    break\n",
    "        chunks.append(text[start:end].strip())\n",
    "        start = end - overlap if end < len(text) else end\n",
    "    return chunks\n",
    "\n",
    "def summarize_reviews(text, summarizer):\n",
    "    if not text or len(text.strip()) == 0:\n",
    "        return \"\"\n",
    "    try:\n",
    "        # Short text - direct summarization\n",
    "        if len(text) <= 1000:\n",
    "            prompt = f\"{GUIDANCE_PROMPT}\\n\\n{text}\"\n",
    "            result = summarizer(prompt, max_length=130, min_length=30, num_beams=2, \n",
    "                              no_repeat_ngram_size=3, do_sample=False, truncation=True)\n",
    "            return result[0]['summary_text']\n",
    "        \n",
    "        # Long text - chunked approach\n",
    "        chunks = split_into_chunks(text)\n",
    "        chunk_summaries = []\n",
    "        for chunk in chunks:\n",
    "            if len(chunk) < 50:\n",
    "                continue\n",
    "            prompt = f\"{GUIDANCE_PROMPT}\\n\\n{chunk}\"\n",
    "            result = summarizer(prompt, max_length=80, min_length=30, num_beams=2,\n",
    "                              no_repeat_ngram_size=3, do_sample=False, truncation=True)\n",
    "            chunk_summaries.append(result[0]['summary_text'])\n",
    "        \n",
    "        if not chunk_summaries:\n",
    "            return \"\"\n",
    "        \n",
    "        # Final pass\n",
    "        combined = ' '.join(chunk_summaries)\n",
    "        if len(combined) > 200:\n",
    "            prompt = f\"{GUIDANCE_PROMPT}\\n\\n{combined}\"\n",
    "            result = summarizer(prompt, max_length=130, min_length=40, num_beams=2,\n",
    "                              no_repeat_ngram_size=3, do_sample=False, truncation=True)\n",
    "            return result[0]['summary_text']\n",
    "        return combined\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "print(\"✓ Summarization functions ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel processing with START_APP_ID support\n",
    "import multiprocessing\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Sort and filter by START_APP_ID\n",
    "combined_df_sorted = combined_df.sort_values('app_id').reset_index(drop=True)\n",
    "\n",
    "if START_APP_ID is not None:\n",
    "    combined_df_sorted = combined_df_sorted[combined_df_sorted['app_id'] >= START_APP_ID].reset_index(drop=True)\n",
    "    print(f\"Starting from app_id {START_APP_ID}\")\n",
    "\n",
    "n_rows = len(combined_df_sorted)\n",
    "print(f\"Processing {n_rows} games\")\n",
    "print(f\"Checkpoint every {CHECKPOINT_EVERY} games\")\n",
    "print(f\"Workers: {NUM_WORKERS}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Initialize\n",
    "if 'reviews_summary' not in combined_df_sorted.columns:\n",
    "    combined_df_sorted['reviews_summary'] = ''\n",
    "\n",
    "# Worker function\n",
    "def process_game(idx):\n",
    "    try:\n",
    "        row_idx = combined_df_sorted.index[idx]\n",
    "        text = combined_df_sorted.loc[row_idx, 'combined_reviews']\n",
    "        app_id = combined_df_sorted.loc[row_idx, 'app_id']\n",
    "        \n",
    "        if not text or len(text.strip()) == 0:\n",
    "            return (idx, app_id, '', 'empty')\n",
    "        \n",
    "        summary = summarize_reviews(text, summarizer)\n",
    "        return (idx, app_id, summary, None)\n",
    "    except Exception as e:\n",
    "        app_id = combined_df_sorted.loc[combined_df_sorted.index[idx], 'app_id']\n",
    "        return (idx, app_id, '', str(e)[:100])\n",
    "\n",
    "# Process in parallel\n",
    "successful = 0\n",
    "failed = 0\n",
    "skipped = 0\n",
    "checkpoint_counter = 0\n",
    "checkpoint_start_id = None\n",
    "checkpoint_end_id = None\n",
    "\n",
    "print(\"Starting...\\n\")\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=NUM_WORKERS) as executor:\n",
    "    futures = {executor.submit(process_game, idx): idx for idx in range(n_rows)}\n",
    "    \n",
    "    with tqdm(total=n_rows, desc=\"Summarizing\", unit=\"game\") as pbar:\n",
    "        for future in as_completed(futures):\n",
    "            idx, app_id, summary, error = future.result()\n",
    "            row_idx = combined_df_sorted.index[idx]\n",
    "            \n",
    "            if checkpoint_counter == 0:\n",
    "                checkpoint_start_id = app_id\n",
    "            checkpoint_end_id = app_id\n",
    "            \n",
    "            if error == 'empty':\n",
    "                skipped += 1\n",
    "            elif error:\n",
    "                failed += 1\n",
    "            else:\n",
    "                combined_df_sorted.loc[row_idx, 'reviews_summary'] = summary\n",
    "                successful += 1\n",
    "            \n",
    "            pbar.update(1)\n",
    "            checkpoint_counter += 1\n",
    "            \n",
    "            # Checkpoint\n",
    "            if checkpoint_counter >= CHECKPOINT_EVERY or idx == n_rows - 1:\n",
    "                checkpoint_file = os.path.join(\n",
    "                    PROCESSED_DIR,\n",
    "                    f'checkpoint_summaries_appid_{checkpoint_start_id:06d}_to_{checkpoint_end_id:06d}.csv'\n",
    "                )\n",
    "                combined_df_sorted[['app_id', 'reviews_summary']].to_csv(checkpoint_file, index=False)\n",
    "                pbar.write(f\"💾 Checkpoint: {os.path.basename(checkpoint_file)}\")\n",
    "                checkpoint_counter = 0\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"✓ Complete!\")\n",
    "print(f\"  Successful: {successful}\")\n",
    "print(f\"  Failed: {failed}\")\n",
    "print(f\"  Skipped: {skipped}\")\n",
    "print(f\"  Success rate: {successful/(n_rows-skipped)*100:.1f}%\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Save final\n",
    "combined_df_sorted[['app_id', 'combined_reviews', 'reviews_summary']].to_csv(FINAL_OUTPUT_FILE, index=False)\n",
    "print(f\"\\n✓ Final saved: {FINAL_OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: Process all remaining rows (uncomment to run)\n",
    "\n",
    "# print(\"Processing all remaining rows...\")\n",
    "# \n",
    "# successful = 0\n",
    "# failed = 0\n",
    "# \n",
    "# # Process rows that don't have summaries yet\n",
    "# rows_to_process = combined_df[combined_df['reviews_summary'] == ''].index\n",
    "# print(f\"Remaining rows to process: {len(rows_to_process)}\")\n",
    "# \n",
    "# for row_idx in tqdm(rows_to_process, desc=\"Summarizing remaining reviews\"):\n",
    "#     combined_text = combined_df.loc[row_idx, 'combined_reviews']\n",
    "#     \n",
    "#     if not combined_text or len(combined_text.strip()) == 0:\n",
    "#         failed += 1\n",
    "#         continue\n",
    "#     \n",
    "#     try:\n",
    "#         summary = summarize_reviews(combined_text, summarizer)\n",
    "#         combined_df.loc[row_idx, 'reviews_summary'] = summary\n",
    "#         successful += 1\n",
    "#     except Exception as e:\n",
    "#         print(f\"\\nError at index {row_idx}: {e}\")\n",
    "#         failed += 1\n",
    "#         continue\n",
    "# \n",
    "# print(\"\\n\" + \"=\" * 60)\n",
    "# print(f\"✓ Full processing complete!\")\n",
    "# print(f\"  Additional successful: {successful}\")\n",
    "# print(f\"  Additional failed: {failed}\")\n",
    "# print(f\"  Total with summaries: {(combined_df['reviews_summary'] != '').sum()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "steam_games_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
