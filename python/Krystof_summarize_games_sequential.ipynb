{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steam Review Summarization - Sequential (Windows & Mac Compatible)\n",
    "\n",
    "Uses smaller, faster model (distilbart-cnn-6-6) with GPU acceleration (CUDA/MPS).\n",
    "Sequential processing prevents memory overflow for overnight runs.\n",
    "\n",
    "**Compatible with:** Windows (CUDA/CPU), Mac (MPS/CPU), Linux (CUDA/CPU)\n",
    "\n",
    "**Run cells in order: 1 â†’ 7**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ All imports loaded\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import pandas as pd\n",
    "import glob, re, string, time, gc, warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "try:\n",
    "    from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "    from tqdm import tqdm\n",
    "    import torch\n",
    "except ImportError:\n",
    "    import subprocess, sys\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"transformers\", \"sentencepiece\", \"torch\", \"tqdm\"])\n",
    "    from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "    from tqdm import tqdm\n",
    "    import torch\n",
    "\n",
    "print(\"âœ“ All imports loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  Model: sshleifer/distilbart-cnn-6-6\n",
      "  App ID range: 4780 to 204180\n",
      "  Sequential processing, Checkpoint every: 10 games\n",
      "  Project root: /Users/radimsoukal/Library/Mobile Documents/com~apple~CloudDocs/VSÌŒE/05. SEMESTR/Text Analytics/R/Steam_Games 2\n",
      "  Output dir:   /Users/radimsoukal/Library/Mobile Documents/com~apple~CloudDocs/VSÌŒE/05. SEMESTR/Text Analytics/R/Steam_Games 2/data/reviews_summary\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Configuration (cross-platform paths)\n",
    "from pathlib import Path\n",
    "\n",
    "# Use smaller, faster 6-6 model\n",
    "# Windows-compatible cache paths\n",
    "if os.name == 'nt':  # Windows\n",
    "    default_local_cache = Path.home() / \"hf_cache\" / \"distilbart-6-6\"\n",
    "    hf_default_cache = Path.home() / \".cache\" / \"huggingface\" / \"hub\" / \"models--sshleifer--distilbart-cnn-6-6\"\n",
    "else:  # Mac/Linux\n",
    "    default_local_cache = Path.home() / \"hf_cache\" / \"distilbart-6-6\"\n",
    "    hf_default_cache = Path.home() / \".cache\" / \"huggingface\" / \"hub\" / \"models--sshleifer--distilbart-cnn-6-6\"\n",
    "\n",
    "if default_local_cache.exists():\n",
    "    MODEL_PATH = str(default_local_cache)\n",
    "elif hf_default_cache.exists():\n",
    "    MODEL_PATH = str(hf_default_cache)\n",
    "else:\n",
    "    MODEL_PATH = \"sshleifer/distilbart-cnn-6-6\"  # smaller & faster than 12-6\n",
    "\n",
    "# Project root detection (works on Windows, Mac, Linux)\n",
    "env_root = os.environ.get(\"STEAM_GAMES_ROOT\")\n",
    "if env_root:\n",
    "    project_root = Path(env_root).expanduser().resolve()\n",
    "else:\n",
    "    here = Path.cwd().resolve()\n",
    "    project_root = None\n",
    "    for cand in [here, *here.parents]:\n",
    "        if cand.name == \"python\":\n",
    "            continue\n",
    "        if (cand / \"data\").exists():\n",
    "            project_root = cand\n",
    "            break\n",
    "    if project_root is None:\n",
    "        project_root = here.parent if here.name == \"python\" else here\n",
    "if project_root.name == \"python\":\n",
    "    project_root = project_root.parent\n",
    "\n",
    "DATA_DIR = (project_root / \"data\").resolve()\n",
    "PROCESSED_DIR = (DATA_DIR / \"processed\").resolve()\n",
    "OUT_DIR = (DATA_DIR / \"reviews_summary\").resolve()\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "START_APP_ID = 293741\n",
    "END_APP_ID   = 942970\n",
    "CHECKPOINT_EVERY = 10\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Platform: {os.name} ({'Windows' if os.name == 'nt' else 'Unix-like'})\")\n",
    "print(f\"  Model: {MODEL_PATH}\")\n",
    "print(f\"  App ID range: {START_APP_ID or 'start'} to {END_APP_ID or 'end'}\")\n",
    "print(f\"  Sequential processing, Checkpoint every: {CHECKPOINT_EVERY} games\")\n",
    "print(f\"  Project root: {project_root}\")\n",
    "print(f\"  Output dir:   {OUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Loaded cache: 2622 games\n",
      "Avg length: 80789 chars\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Load data with cache (same pattern as combine_reviews_working_edit)\n",
    "cache_file = PROCESSED_DIR / 'combined_reviews_cache.pkl'\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "combined_df = None\n",
    "\n",
    "if cache_file.exists():\n",
    "    try:\n",
    "        obj = pd.read_pickle(cache_file)\n",
    "        if isinstance(obj, pd.DataFrame) and 'combined_reviews' in obj.columns:\n",
    "            combined_df = obj\n",
    "            print(f\"âœ“ Loaded cache: {len(combined_df)} games\")\n",
    "        else:\n",
    "            print(\"Cache invalid -> will rebuild\")\n",
    "            cache_file.unlink()\n",
    "    except Exception as e:\n",
    "        print(f\"Cache error: {e} -> will rebuild\")\n",
    "        try:\n",
    "            cache_file.unlink()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "if combined_df is None:\n",
    "    print(\"Loading raw CSV files...\")\n",
    "    csv_files = sorted(glob.glob(str(DATA_DIR / 'raw' / 'app_reviews_*.csv')))\n",
    "    if not csv_files:\n",
    "        raise FileNotFoundError(f\"No input files in {DATA_DIR / 'raw'}\")\n",
    "    dfs = [pd.read_csv(f) for f in csv_files]\n",
    "    combined_df = pd.concat(dfs, ignore_index=True)\n",
    "    print(f\"âœ“ Loaded {len(csv_files)} files, {len(combined_df)} games\")\n",
    "    \n",
    "    def clean_text(text):\n",
    "        if not isinstance(text, str) or not text.strip():\n",
    "            return None\n",
    "        text = re.sub(r'<[^>]+>', '', text)\n",
    "        text = re.sub(r'http[s]?://\\S+|www\\.\\S+', '', text)\n",
    "        text = re.sub(r'[\\r\\n\\t]+', ' ', text)\n",
    "        text = re.sub(r'[\\U00010000-\\U0010ffff]', '', text)\n",
    "        allowed = set(string.ascii_letters + string.digits + ' .,!?\\'-:/()[]')\n",
    "        text = ''.join(ch for ch in text if ch in allowed)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return text if text else None\n",
    "    \n",
    "    def combine_reviews(row):\n",
    "        reviews = []\n",
    "        for i in range(1, 101):\n",
    "            rv = row.get(f'review_{i}')\n",
    "            if rv and isinstance(rv, str):\n",
    "                cleaned = clean_text(rv)\n",
    "                if cleaned:\n",
    "                    reviews.append(cleaned)\n",
    "        unique = list(dict.fromkeys(reviews))\n",
    "        return ' [SEP] '.join(unique) if unique else ''\n",
    "    \n",
    "    combined_df['combined_reviews'] = combined_df.apply(combine_reviews, axis=1)\n",
    "    combined_df.to_pickle(cache_file)\n",
    "    print(\"âœ“ Cleaned and cached\")\n",
    "\n",
    "print(f\"Avg length: {combined_df['combined_reviews'].str.len().mean():.0f} chars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: sshleifer/distilbart-cnn-6-6\n",
      "âœ“ Downloaded from Hugging Face Hub\n",
      "âœ“ Device: MPS (Apple Silicon)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Summarizer ready\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Load model with cross-platform GPU support (Windows CUDA / Mac MPS / CPU fallback)\n",
    "def _resolve_hub_snapshot_dir(root_dir):\n",
    "    snapshots_dir = os.path.join(root_dir, \"snapshots\")\n",
    "    refs_main = os.path.join(root_dir, \"refs\", \"main\")\n",
    "    if os.path.isfile(refs_main):\n",
    "        try:\n",
    "            with open(refs_main, \"r\") as f:\n",
    "                commit = f.read().strip()\n",
    "            cand = os.path.join(snapshots_dir, commit)\n",
    "            if os.path.isdir(cand):\n",
    "                return cand\n",
    "        except:\n",
    "            pass\n",
    "    try:\n",
    "        candidates = [d for d in glob.glob(os.path.join(snapshots_dir, \"*\")) if os.path.isdir(d)]\n",
    "        if candidates:\n",
    "            candidates.sort(key=lambda d: os.path.getmtime(d), reverse=True)\n",
    "            return candidates[0]\n",
    "    except:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "def _looks_like_model_dir(path):\n",
    "    must_have = [\"config.json\", \"pytorch_model.bin\", \"tokenizer_config.json\"]\n",
    "    try:\n",
    "        files = set(os.listdir(path))\n",
    "        return any(m in files for m in must_have)\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "# Cross-platform device detection: Windows (CUDA) / Mac (MPS) / Fallback (CPU)\n",
    "def detect_device():\n",
    "    \"\"\"Detect best available device: CUDA (Windows/Linux) > MPS (Mac) > CPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\"), torch.float16, \"CUDA (GPU)\"\n",
    "    if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\"), torch.float32, \"MPS (Apple Silicon)\"\n",
    "    return torch.device(\"cpu\"), torch.float32, \"CPU\"\n",
    "\n",
    "print(f\"Loading model from: {MODEL_PATH}\")\n",
    "is_local_path = os.path.exists(MODEL_PATH)\n",
    "\n",
    "load_dir = None\n",
    "if is_local_path:\n",
    "    if os.path.basename(MODEL_PATH).startswith(\"models--\"):\n",
    "        candidate = _resolve_hub_snapshot_dir(MODEL_PATH)\n",
    "        if candidate and _looks_like_model_dir(candidate):\n",
    "            load_dir = candidate\n",
    "    if load_dir is None and _looks_like_model_dir(MODEL_PATH):\n",
    "        load_dir = MODEL_PATH\n",
    "\n",
    "if load_dir:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(load_dir, local_files_only=True)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(load_dir, local_files_only=True)\n",
    "    print(f\"âœ“ Loaded from local cache: {load_dir}\")\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_PATH)\n",
    "    print(\"âœ“ Downloaded from Hugging Face Hub\")\n",
    "\n",
    "device, dtype, dev_name = detect_device()\n",
    "print(f\"âœ“ Device: {dev_name}\")\n",
    "model = model.to(device)\n",
    "if dtype == torch.float16:\n",
    "    model = model.half()\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer, device=-1)\n",
    "print(\"âœ“ Summarizer ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Summarization functions ready\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Summarization functions with content filtering\n",
    "GUIDANCE_PROMPT = (\n",
    "    \"Summarize the following user reviews into a concise, neutral third-person description of the game. \"\n",
    "    \"Focus on: genre, core gameplay loop and mechanics, key features/modes (campaign/multiplayer/co-op), \"\n",
    "    \"difficulty/learning curve, performance/technical notes. Avoid slang, memes, and repetition.\\n\\nReviews:\\n\"\n",
    ")\n",
    "\n",
    "KEYWORDS_RE = re.compile(\n",
    "    r\"gameplay|mechanic|combat|gun|weapon|campaign|story|mode|co-?op|multiplayer|\"\n",
    "    r\"graphics|performance|bug|optim|difficulty|progression|content|map|level|class|rank\",\n",
    "    re.I\n",
    ")\n",
    "\n",
    "def keep_relevant_sentences(text):\n",
    "    sents = re.split(r\"(?<=[.!?])\\s+\", text)\n",
    "    kept = [s for s in sents if KEYWORDS_RE.search(s)]\n",
    "    filtered = \" \".join(kept).strip()\n",
    "    return filtered if len(filtered) >= 300 else text\n",
    "\n",
    "def split_into_chunks(text, max_chunk=1500, overlap=100):\n",
    "    if not text or len(text) <= max_chunk:\n",
    "        return [text] if text else []\n",
    "    chunks, start = [], 0\n",
    "    while start < len(text):\n",
    "        end = start + max_chunk\n",
    "        if end < len(text):\n",
    "            window = text[start:end]\n",
    "            cut = max(window.rfind(\". \"), window.rfind(\"! \"), window.rfind(\" [SEP] \"))\n",
    "            if cut != -1:\n",
    "                end = start + cut + 2\n",
    "        chunk = text[start:end].strip()\n",
    "        if chunk:\n",
    "            chunks.append(chunk)\n",
    "        start = end - overlap if end < len(text) else end\n",
    "    return chunks\n",
    "\n",
    "def token_budget(n_chars, cap=96):\n",
    "    return max(32, min(cap, n_chars // 4))\n",
    "\n",
    "def generate_summary(text, cap=96):\n",
    "    if not text or not text.strip():\n",
    "        return \"\"\n",
    "    max_new = token_budget(len(text), cap=cap)\n",
    "    with torch.inference_mode():\n",
    "        out = summarizer(text, max_new_tokens=max_new, num_beams=1, \n",
    "                        do_sample=False, truncation=True, batch_size=1)[0][\"summary_text\"]\n",
    "    return out\n",
    "\n",
    "def summarize_reviews(text):\n",
    "    if not text or not text.strip():\n",
    "        return \"\"\n",
    "    base = keep_relevant_sentences(text)\n",
    "    if len(base) <= 800:\n",
    "        guided = f\"{GUIDANCE_PROMPT}{base}\"\n",
    "        return generate_summary(guided, cap=64)\n",
    "    parts = []\n",
    "    for ch in split_into_chunks(base, max_chunk=1500, overlap=100):\n",
    "        guided = f\"{GUIDANCE_PROMPT}{ch}\"\n",
    "        parts.append(generate_summary(guided, cap=64))\n",
    "    combined = \" \".join(p for p in parts if p).strip()\n",
    "    if len(combined) < 200:\n",
    "        return combined\n",
    "    final_guided = f\"{GUIDANCE_PROMPT}{combined}\"\n",
    "    return generate_summary(final_guided, cap=96)\n",
    "\n",
    "print(\"âœ“ Summarization functions ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 577 games (SEQUENTIAL mode)\n",
      "------------------------------------------------------------\n",
      "\n",
      "Batch 1/58  (app_id 4780â€“6060)  size=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 1:   0%|          | 0/10 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['early_stopping', 'length_penalty']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Batch 1:  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [08:18<19:23, 166.26s/it]"
     ]
    }
   ],
   "source": [
    "# Cell 6: Sequential processing with memory management\n",
    "df_sorted = combined_df.sort_values('app_id').reset_index(drop=True)\n",
    "if START_APP_ID is not None:\n",
    "    df_sorted = df_sorted[df_sorted['app_id'] >= START_APP_ID]\n",
    "if END_APP_ID is not None:\n",
    "    df_sorted = df_sorted[df_sorted['app_id'] <= END_APP_ID]\n",
    "df_sorted = df_sorted.reset_index(drop=True)\n",
    "\n",
    "n_rows = len(df_sorted)\n",
    "print(f\"\\nProcessing {n_rows} games (SEQUENTIAL mode)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "if n_rows == 0:\n",
    "    print(\"No rows to process\")\n",
    "else:\n",
    "    if 'reviews_summary' not in df_sorted.columns:\n",
    "        df_sorted['reviews_summary'] = ''\n",
    "    \n",
    "    ok = fail = skip = 0\n",
    "    batch_size = max(1, CHECKPOINT_EVERY)\n",
    "    overall_start_id = int(df_sorted.iloc[0]['app_id'])\n",
    "    overall_end_id = int(df_sorted.iloc[-1]['app_id'])\n",
    "    total_batches = (n_rows + batch_size - 1) // batch_size\n",
    "    \n",
    "    for b in range(total_batches):\n",
    "        s = b * batch_size\n",
    "        e = min(s + batch_size, n_rows)\n",
    "        batch = df_sorted.iloc[s:e]\n",
    "        batch_start_id = int(batch.iloc[0]['app_id'])\n",
    "        batch_end_id = int(batch.iloc[-1]['app_id'])\n",
    "        \n",
    "        print(f\"\\nBatch {b+1}/{total_batches}  (app_id {batch_start_id}â€“{batch_end_id})  size={len(batch)}\")\n",
    "        \n",
    "        for idx, row in tqdm(batch.iterrows(), total=len(batch), desc=f\"Batch {b+1}\"):\n",
    "            text = row.get('combined_reviews', '')\n",
    "            app_id = int(row['app_id'])\n",
    "            \n",
    "            if not text or not text.strip():\n",
    "                skip += 1\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                summary = summarize_reviews(text)\n",
    "                df_sorted.loc[idx, 'reviews_summary'] = summary\n",
    "                ok += 1\n",
    "            except Exception as ex:\n",
    "                fail += 1\n",
    "                print(f\"  âš ï¸  app_id {app_id}: {ex}\")\n",
    "        \n",
    "        ck_path = OUT_DIR / f\"checkpoint_{batch_start_id:06d}_to_{batch_end_id:06d}.csv\"\n",
    "        df_sorted.loc[s:e-1, ['app_id', 'reviews_summary']].to_csv(ck_path, index=False)\n",
    "        print(f\"  ðŸ’¾ Saved {ck_path.name} | ok={ok}, fail={fail}, skip={skip}\")\n",
    "        \n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        time.sleep(0.2)\n",
    "        print(f\"  ðŸ§¹ Memory cleanup complete\")\n",
    "    \n",
    "    final_name = f\"review_summaries_COMPLETE_{overall_start_id:06d}_to_{overall_end_id:06d}.csv\"\n",
    "    final_path = OUT_DIR / final_name\n",
    "    df_sorted[['app_id', 'combined_reviews', 'reviews_summary']].to_csv(final_path, index=False)\n",
    "    \n",
    "    print(f\"\\nâœ“ Complete: ok={ok}, fail={fail}, skip={skip}\")\n",
    "    print(f\"âœ“ Saved: {final_path}\")\n",
    "    gc.collect()\n",
    "    print(\"âœ“ Final cleanup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Notes\n",
    "\n",
    "**Platform Compatibility:**\n",
    "- âœ… **Windows**: CUDA GPU (NVIDIA) or CPU\n",
    "- âœ… **Mac**: MPS (M1/M2/M3) or CPU  \n",
    "- âœ… **Linux**: CUDA GPU or CPU\n",
    "\n",
    "**Sequential vs Parallel:**\n",
    "- âœ… Sequential: Stable, predictable memory ~2-4GB  \n",
    "- âœ… No crashes overnight  \n",
    "- âœ… GPU acceleration when available (2-5x faster than CPU)  \n",
    "- âœ… Smaller model (6-6 vs 12-6) = faster processing  \n",
    "\n",
    "**Speed estimates by device:**\n",
    "- ~20-30 sec/game on **NVIDIA GPU** (Windows/Linux CUDA)\n",
    "- ~30-40 sec/game on **M1/M2 Mac** (MPS)  \n",
    "- ~60-90 sec/game on **CPU** (any platform)\n",
    "\n",
    "**Memory optimizations:**\n",
    "- Content filtering reduces input size  \n",
    "- Sequential processing (no parallel overhead)  \n",
    "- Garbage collection after each batch  \n",
    "- Smaller model footprint  \n",
    "\n",
    "**Windows-specific notes:**\n",
    "- If you have an NVIDIA GPU, ensure CUDA is installed\n",
    "- CPU mode works on any Windows machine (just slower)\n",
    "- Monitor GPU usage in Task Manager > Performance > GPU\n",
    "\n",
    "**If you need more speed:**\n",
    "- Use the parallel version for small batches  \n",
    "- Process in smaller ranges (e.g., 100-200 games)  \n",
    "- This sequential version is best for overnight/long runs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "steam_games_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
