{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steam Review Summarization\n",
    "\n",
    "Generates neutral game descriptions from 100+ user reviews using distilbart-cnn-12-6.\n",
    "\n",
    "**Run cells in order: 1 → 7**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All imports loaded (TOKENIZERS_PARALLELISM=false)\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # avoid tokenizer threading/fork issues\n",
    "\n",
    "import pandas as pd\n",
    "import glob\n",
    "import re\n",
    "import string\n",
    "import pickle\n",
    "import time\n",
    "import multiprocessing\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Install transformers if needed\n",
    "try:\n",
    "    from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "    from tqdm import tqdm\n",
    "    import torch\n",
    "except ImportError:\n",
    "    print(\"Installing required packages...\")\n",
    "    import subprocess\n",
    "    import sys\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"transformers\", \"sentencepiece\", \"torch\", \"tqdm\"])\n",
    "    from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "    from tqdm import tqdm\n",
    "    import torch\n",
    "\n",
    "print(\"✓ All imports loaded (TOKENIZERS_PARALLELISM=false)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  Model: /Users/radimsoukal/.cache/huggingface/hub/models--sshleifer--distilbart-cnn-12-6\n",
      "  App ID range: 7000 to 12000\n",
      "  Workers: 9, Checkpoint every: 25\n",
      "  Project root: /Users/radimsoukal/Library/Mobile Documents/com~apple~CloudDocs/VŠE/05. SEMESTR/Text Analytics/R/Steam_Games 2\n",
      "  Data dir:     /Users/radimsoukal/Library/Mobile Documents/com~apple~CloudDocs/VŠE/05. SEMESTR/Text Analytics/R/Steam_Games 2/data\n",
      "  Processed dir:/Users/radimsoukal/Library/Mobile Documents/com~apple~CloudDocs/VŠE/05. SEMESTR/Text Analytics/R/Steam_Games 2/data/processed\n"
     ]
    }
   ],
   "source": [
    "# Configuration – adaptive root detection (never inside /python)\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Model path setup (unchanged) ---\n",
    "default_local_cache = Path.home() / \"hf_cache\" / \"distilbart\"\n",
    "hf_default_cache = Path.home() / \".cache\" / \"huggingface\" / \"hub\" / \"models--sshleifer--distilbart-cnn-12-6\"\n",
    "\n",
    "if default_local_cache.exists():\n",
    "    MODEL_PATH = str(default_local_cache)\n",
    "elif hf_default_cache.exists():\n",
    "    MODEL_PATH = str(hf_default_cache)\n",
    "else:\n",
    "    MODEL_PATH = \"sshleifer/distilbart-cnn-12-6\"  # fallback (downloads if missing)\n",
    "\n",
    "# --- Project root detection ---\n",
    "# Priority:\n",
    "# 1) STEAM_GAMES_ROOT env var, if provided\n",
    "# 2) Walk up from CWD to the first folder that has a \"data\" subfolder and is NOT named \"python\"\n",
    "# 3) If still not found, prefer parent of a trailing \"/python\" folder; else use CWD\n",
    "env_root = os.environ.get(\"STEAM_GAMES_ROOT\")\n",
    "if env_root:\n",
    "    project_root = Path(env_root).expanduser().resolve()\n",
    "else:\n",
    "    here = Path.cwd().resolve()\n",
    "    project_root = None\n",
    "    for cand in [here, *here.parents]:\n",
    "        if cand.name == \"python\":\n",
    "            continue\n",
    "        if (cand / \"data\").exists():\n",
    "            project_root = cand\n",
    "            break\n",
    "    if project_root is None:\n",
    "        project_root = here.parent if here.name == \"python\" else here\n",
    "\n",
    "# Final safety: never allow .../python/data\n",
    "if project_root.name == \"python\":\n",
    "    project_root = project_root.parent\n",
    "\n",
    "# --- Data directories ---\n",
    "DATA_DIR = (project_root / \"data\").resolve()\n",
    "PROCESSED_DIR = (DATA_DIR / \"processed\").resolve()\n",
    "\n",
    "# --- Processing range ---\n",
    "START_APP_ID = 7000   # None = start from beginning\n",
    "END_APP_ID   = 12000  # None = process all\n",
    "\n",
    "# --- Processing settings ---\n",
    "NUM_WORKERS = (os.cpu_count() or 4) - 1\n",
    "CHECKPOINT_EVERY = 25\n",
    "\n",
    "# --- Display configuration ---\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Model: {MODEL_PATH}\")\n",
    "print(f\"  App ID range: {START_APP_ID or 'start'} to {END_APP_ID or 'end'}\")\n",
    "print(f\"  Workers: {NUM_WORKERS}, Checkpoint every: {CHECKPOINT_EVERY}\")\n",
    "print(f\"  Project root: {project_root}\")\n",
    "print(f\"  Data dir:     {DATA_DIR}\")\n",
    "print(f\"  Processed dir:{PROCESSED_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean cache (run this if you get KeyError about 'combined_reviews')\n",
    "cache_file = os.path.join(PROCESSED_DIR, 'combined_reviews_cache.pkl')\n",
    "if os.path.exists(cache_file):\n",
    "    os.remove(cache_file)\n",
    "    print(\"✓ Cache deleted - will rebuild on next cell\")\n",
    "else:\n",
    "    print(\"✓ No cache to delete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded cache: 2622 games\n",
      "Avg length: 80789 chars\n"
     ]
    }
   ],
   "source": [
    "# Load and combine CSV files (robust cache handling)\n",
    "cache_file = os.path.join(PROCESSED_DIR, 'combined_reviews_cache.pkl')\n",
    "os.makedirs(PROCESSED_DIR, exist_ok=True)\n",
    "\n",
    "combined_df = None  # ensure we rebuild a proper DataFrame in this cell\n",
    "\n",
    "# 1) Try to load a valid cache\n",
    "if os.path.exists(cache_file):\n",
    "    try:\n",
    "        obj = pd.read_pickle(cache_file)\n",
    "        if isinstance(obj, pd.DataFrame) and 'combined_reviews' in obj.columns:\n",
    "            combined_df = obj\n",
    "            print(f\"✓ Loaded cache: {len(combined_df)} games\")\n",
    "        elif isinstance(obj, pd.Series):\n",
    "            print(\"Cache contains a Series -> rebuilding DataFrame from raw CSVs and reusing combined text…\")\n",
    "            # Load raw CSVs and attach the series as the combined_reviews column\n",
    "            csv_files = sorted(glob.glob(os.path.join(DATA_DIR, 'raw', 'app_reviews_*.csv')))\n",
    "            dfs = [pd.read_csv(f) for f in csv_files]\n",
    "            raw_df = pd.concat(dfs, ignore_index=True)\n",
    "            series_vals = obj.reset_index(drop=True)\n",
    "            if len(raw_df) == len(series_vals):\n",
    "                raw_df['combined_reviews'] = series_vals\n",
    "                combined_df = raw_df\n",
    "                combined_df.to_pickle(cache_file)  # overwrite cache with correct format\n",
    "                print(f\"✓ Rewrote cache as DataFrame: {len(combined_df)} games\")\n",
    "            else:\n",
    "                print(\"Series length mismatch with raw CSVs -> discarding cache\")\n",
    "                os.remove(cache_file)\n",
    "        else:\n",
    "            print(\"Cache invalid type -> discarding cache\")\n",
    "            os.remove(cache_file)\n",
    "    except Exception as e:\n",
    "        print(f\"Cache load error: {e} -> discarding cache\")\n",
    "        try:\n",
    "            os.remove(cache_file)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "# 2) Build from raw if needed\n",
    "if combined_df is None:\n",
    "    print(\"Loading raw CSV files and building combined reviews…\")\n",
    "    csv_files = sorted(glob.glob(os.path.join(DATA_DIR, 'raw', 'app_reviews_*.csv')))\n",
    "    if not csv_files:\n",
    "        raise FileNotFoundError(f\"No input files found in {os.path.join(DATA_DIR, 'raw')} (expected app_reviews_*.csv)\")\n",
    "    dfs = [pd.read_csv(f) for f in csv_files]\n",
    "    combined_df = pd.concat(dfs, ignore_index=True)\n",
    "    print(f\"✓ Loaded {len(csv_files)} files, {len(combined_df)} games\")\n",
    "\n",
    "    def clean_text(text):\n",
    "        if not isinstance(text, str) or not text.strip():\n",
    "            return None\n",
    "        text = re.sub(r'<[^>]+>', '', text)\n",
    "        text = re.sub(r'http[s]?://\\S+|www\\.\\S+', '', text)\n",
    "        text = re.sub(r'[\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\\U0001F680-\\U0001F6FF\\U0001F1E0-\\U0001F1FF]+', '', text)\n",
    "        text = text.replace('\\r', ' ').replace('\\n', ' ').replace('\\t', ' ')\n",
    "        allowed_chars = set(string.ascii_letters + string.digits + ' .,!?\\'-')\n",
    "        text = ''.join(char for char in text if char in allowed_chars)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return text if text else None\n",
    "\n",
    "    def combine_reviews(row):\n",
    "        reviews = []\n",
    "        for i in range(1, 101):\n",
    "            review = row.get(f'review_{i}')\n",
    "            if review and isinstance(review, str):\n",
    "                cleaned = clean_text(review)\n",
    "                if cleaned:\n",
    "                    reviews.append(cleaned)\n",
    "        unique = list(dict.fromkeys(reviews))\n",
    "        return ' [SEP] '.join(unique) if unique else ''\n",
    "\n",
    "    combined_df['combined_reviews'] = combined_df.apply(combine_reviews, axis=1)\n",
    "    combined_df.to_pickle(cache_file)\n",
    "    print(\"✓ Cleaned and cached reviews\")\n",
    "\n",
    "# 3) Final sanity check and stats\n",
    "if not isinstance(combined_df, pd.DataFrame) or 'combined_reviews' not in combined_df.columns:\n",
    "    raise RuntimeError(\"combined_df is not a DataFrame with a 'combined_reviews' column. Please re-run this cell.\")\n",
    "\n",
    "print(f\"Avg length: {combined_df['combined_reviews'].str.len().mean():.0f} chars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /Users/radimsoukal/.cache/huggingface/hub/models--sshleifer--distilbart-cnn-12-6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded model from local cache: /Users/radimsoukal/.cache/huggingface/hub/models--sshleifer--distilbart-cnn-12-6/snapshots/a4f8f3ea906ed274767e9906dbaede7531d660ff\n",
      "✓ Model loaded successfully\n",
      "✓ Summarizer defined: <class 'transformers.pipelines.text2text_generation.SummarizationPipeline'>\n"
     ]
    }
   ],
   "source": [
    "# BLOCK 4: Load model from local cache or Hugging Face if missing\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "import os\n",
    "\n",
    "def _resolve_hub_snapshot_dir(root_dir: str) -> str | None:\n",
    "    \"\"\"If MODEL_PATH is a Hugging Face hub cache root (models--org--model),\n",
    "    resolve it to an actual snapshot dir containing the model files.\"\"\"\n",
    "    snapshots_dir = os.path.join(root_dir, \"snapshots\")\n",
    "    refs_main = os.path.join(root_dir, \"refs\", \"main\")\n",
    "\n",
    "    # Prefer the commit hash in refs/main if present\n",
    "    if os.path.isfile(refs_main):\n",
    "        try:\n",
    "            with open(refs_main, \"r\", encoding=\"utf-8\") as f:\n",
    "                commit = f.read().strip()\n",
    "            cand = os.path.join(snapshots_dir, commit)\n",
    "            if os.path.isdir(cand):\n",
    "                return cand\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # Else pick the newest snapshot\n",
    "    try:\n",
    "        candidates = [d for d in glob.glob(os.path.join(snapshots_dir, \"*\")) if os.path.isdir(d)]\n",
    "        if candidates:\n",
    "            candidates.sort(key=lambda d: os.path.getmtime(d), reverse=True)\n",
    "            return candidates[0]\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "def _looks_like_model_dir(path: str) -> bool:\n",
    "    # Any of these files in the directory is good enough\n",
    "    must_have = [\"config.json\", \"pytorch_model.bin\", \"tokenizer_config.json\", \"vocab.json\", \"merges.txt\"]\n",
    "    try:\n",
    "        files = set(os.listdir(path))\n",
    "        return any(m in files for m in must_have)\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "print(f\"Loading model from: {MODEL_PATH}\")\n",
    "is_local_path = os.path.exists(MODEL_PATH)\n",
    "\n",
    "try:\n",
    "    load_dir = None\n",
    "    if is_local_path:\n",
    "        # If it's a hub cache root, resolve to a snapshot subdir\n",
    "        if os.path.basename(MODEL_PATH).startswith(\"models--\"):\n",
    "            candidate = _resolve_hub_snapshot_dir(MODEL_PATH)\n",
    "            if candidate and _looks_like_model_dir(candidate):\n",
    "                load_dir = candidate\n",
    "        # If it's already a real model folder with files, use it directly\n",
    "        if load_dir is None and _looks_like_model_dir(MODEL_PATH):\n",
    "            load_dir = MODEL_PATH\n",
    "\n",
    "    if load_dir:\n",
    "        # Local-only load\n",
    "        tokenizer = AutoTokenizer.from_pretrained(load_dir, local_files_only=True)\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(load_dir, local_files_only=True)\n",
    "        print(f\"✓ Loaded model from local cache: {load_dir}\")\n",
    "    else:\n",
    "        # Use Hub model ID (downloads if missing)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_PATH)\n",
    "        print(\"✓ Downloaded or loaded from Hugging Face Hub\")\n",
    "\n",
    "    # Define summarizer (CPU by default; you can move model to MPS later if desired)\n",
    "    global summarizer\n",
    "    summarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer, device=-1)\n",
    "\n",
    "    print(\"✓ Model loaded successfully\")\n",
    "    print(f\"✓ Summarizer defined: {type(summarizer)}\")\n",
    "\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"❌ Failed to load model from {MODEL_PATH}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Summarization functions ready\n"
     ]
    }
   ],
   "source": [
    "# BLOCK 5: Summarization functions with chunking and guidance\n",
    "\n",
    "GUIDANCE_PROMPT = \"\"\"Summarize the following user reviews into a concise, neutral third-person description of the game. Focus on: genre, gameplay mechanics, core features or modes, campaign or multiplayer aspects, and overall atmosphere or difficulty. Avoid opinions, memes, and repetition.\n",
    "\n",
    "Reviews:\"\"\"\n",
    "\n",
    "\n",
    "def split_into_chunks(text, max_chunk_size=3500, overlap=200):\n",
    "    if not text or len(text) <= max_chunk_size:\n",
    "        return [text] if text else []\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + max_chunk_size\n",
    "        if end < len(text):\n",
    "            for sep in ['. ', '! ', '? ', ' [SEP] ']:\n",
    "                last_sep = text[start:end].rfind(sep)\n",
    "                if last_sep != -1:\n",
    "                    end = start + last_sep + len(sep)\n",
    "                    break\n",
    "        chunks.append(text[start:end].strip())\n",
    "        start = end - overlap if end < len(text) else end\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def summarize_reviews(text, summarizer):\n",
    "    if not text or len(text.strip()) == 0:\n",
    "        return \"\"\n",
    "    try:\n",
    "        # Short text - direct summarization\n",
    "        if len(text) <= 1000:\n",
    "            prompt = f\"{GUIDANCE_PROMPT}\\n\\n{text}\"\n",
    "            result = summarizer(\n",
    "                prompt,\n",
    "                max_length=130,\n",
    "                min_length=30,\n",
    "                num_beams=2,\n",
    "                no_repeat_ngram_size=3,\n",
    "                do_sample=False,\n",
    "                truncation=True,\n",
    "            )\n",
    "            return result[0]['summary_text']\n",
    "\n",
    "        # Long text - chunked approach\n",
    "        chunks = split_into_chunks(text)\n",
    "        chunk_summaries = []\n",
    "        for chunk in chunks:\n",
    "            if len(chunk) < 50:\n",
    "                continue\n",
    "            prompt = f\"{GUIDANCE_PROMPT}\\n\\n{chunk}\"\n",
    "            result = summarizer(\n",
    "                prompt,\n",
    "                max_length=80,\n",
    "                min_length=30,\n",
    "                num_beams=2,\n",
    "                no_repeat_ngram_size=3,\n",
    "                do_sample=False,\n",
    "                truncation=True,\n",
    "            )\n",
    "            chunk_summaries.append(result[0]['summary_text'])\n",
    "\n",
    "        if not chunk_summaries:\n",
    "            return \"\"\n",
    "\n",
    "        # Final pass\n",
    "        combined = ' '.join(chunk_summaries)\n",
    "        if len(combined) > 200:\n",
    "            prompt = f\"{GUIDANCE_PROMPT}\\n\\n{combined}\"\n",
    "            result = summarizer(\n",
    "                prompt,\n",
    "                max_length=130,\n",
    "                min_length=40,\n",
    "                num_beams=2,\n",
    "                no_repeat_ngram_size=3,\n",
    "                do_sample=False,\n",
    "                truncation=True,\n",
    "            )\n",
    "            return result[0]['summary_text']\n",
    "        return combined\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "print(\"✓ Summarization functions ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 66 games (workers: 9)\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarizing 7000-8600: 100%|██████████| 25/25 [26:07<00:00, 62.71s/game]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 checkpoint_007000_to_008600.csv | batch done: 25 ok, 0 failed, 0 skipped (cumulative)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarizing 8800-10180:  60%|██████    | 15/25 [18:10<11:20, 68.01s/game]"
     ]
    }
   ],
   "source": [
    "# BLOCK 6: Parallel processing with checkpoints (robust + ordered checkpoints)\n",
    "os.makedirs(\"data/reviews_summary\", exist_ok=True)\n",
    "\n",
    "# Ensure tokenizer doesn't parallelize internally when we use threads\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Fallback: ensure summarizer exists (in case Block 4 wasn't run in this session)\n",
    "if 'summarizer' not in globals() or summarizer is None:\n",
    "    print(\"Summarizer not found in session. Loading from local cache (fallback)...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, local_files_only=True)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_PATH, local_files_only=True)\n",
    "    summarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer, device=-1)\n",
    "    print(\"✓ Summarizer loaded (fallback)\")\n",
    "\n",
    "# Filter by app_id range and sort\n",
    "df_sorted = combined_df.sort_values('app_id').reset_index(drop=True)\n",
    "if START_APP_ID is not None:\n",
    "    df_sorted = df_sorted[df_sorted['app_id'] >= START_APP_ID]\n",
    "if END_APP_ID is not None:\n",
    "    df_sorted = df_sorted[df_sorted['app_id'] <= END_APP_ID]\n",
    "df_sorted = df_sorted.reset_index(drop=True)\n",
    "\n",
    "n_rows = len(df_sorted)\n",
    "print(f\"\\nProcessing {n_rows} games (workers: {NUM_WORKERS})\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "if n_rows == 0:\n",
    "    print(\"No rows to process for the selected range.\")\n",
    "else:\n",
    "    if 'reviews_summary' not in df_sorted.columns:\n",
    "        df_sorted['reviews_summary'] = ''\n",
    "\n",
    "    # Lock summarizer calls to avoid thread-safety issues\n",
    "    from threading import Lock\n",
    "    summarize_lock = Lock()\n",
    "\n",
    "    def process_game(idx):\n",
    "        try:\n",
    "            text = df_sorted.loc[df_sorted.index[idx], 'combined_reviews']\n",
    "            app_id = int(df_sorted.loc[df_sorted.index[idx], 'app_id'])\n",
    "            if not text or len(text.strip()) == 0:\n",
    "                return (idx, app_id, '', 'empty')\n",
    "            # Guard model call with a lock\n",
    "            with summarize_lock:\n",
    "                summary = summarize_reviews(text, summarizer)\n",
    "            return (idx, app_id, summary, None)\n",
    "        except Exception as e:\n",
    "            app_id = int(df_sorted.loc[df_sorted.index[idx], 'app_id'])\n",
    "            return (idx, app_id, '', str(e)[:200])\n",
    "\n",
    "    successful = failed = skipped = 0\n",
    "\n",
    "    # Process in ordered batches so checkpoint filenames reflect contiguous app_id ranges\n",
    "    batch_size = CHECKPOINT_EVERY if CHECKPOINT_EVERY and CHECKPOINT_EVERY > 0 else 100\n",
    "\n",
    "    for start in range(0, n_rows, batch_size):\n",
    "        end = min(start + batch_size, n_rows)\n",
    "        batch_indices = list(range(start, end))\n",
    "        batch_start_id = int(df_sorted.iloc[start]['app_id'])\n",
    "        batch_end_id = int(df_sorted.iloc[end - 1]['app_id'])\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=NUM_WORKERS) as executor:\n",
    "            futures = {executor.submit(process_game, idx): idx for idx in batch_indices}\n",
    "            with tqdm(total=len(batch_indices), desc=f\"Summarizing {batch_start_id}-{batch_end_id}\", unit=\"game\") as pbar:\n",
    "                for future in as_completed(futures):\n",
    "                    idx, app_id, summary, error = future.result()\n",
    "                    row_idx = df_sorted.index[idx]\n",
    "\n",
    "                    if error == 'empty':\n",
    "                        skipped += 1\n",
    "                    elif error:\n",
    "                        failed += 1\n",
    "                        pbar.write(f\"Error @ app_id {app_id}: {error}\")\n",
    "                    else:\n",
    "                        df_sorted.loc[row_idx, 'reviews_summary'] = summary\n",
    "                        successful += 1\n",
    "\n",
    "                    pbar.update(1)\n",
    "\n",
    "        # Write ordered checkpoint for this contiguous batch\n",
    "        checkpoint_path = os.path.join(\n",
    "            \"data/reviews_summary\",\n",
    "            f\"checkpoint_{batch_start_id:06d}_to_{batch_end_id:06d}.csv\",\n",
    "        )\n",
    "        df_sorted.loc[start:end - 1, ['app_id', 'reviews_summary']].to_csv(checkpoint_path, index=False)\n",
    "        print(f\"💾 {os.path.basename(checkpoint_path)} | batch done: {successful} ok, {failed} failed, {skipped} skipped (cumulative)\")\n",
    "\n",
    "    # Save final with full app_id range in filename\n",
    "    overall_start_id = int(df_sorted.iloc[0]['app_id'])\n",
    "    overall_end_id = int(df_sorted.iloc[-1]['app_id'])\n",
    "    final_filename = f\"review_summaries_COMPLETE_{overall_start_id:06d}_to_{overall_end_id:06d}.csv\"\n",
    "    final_path = os.path.join(\"data/reviews_summary\", final_filename)\n",
    "\n",
    "    df_sorted[['app_id', 'combined_reviews', 'reviews_summary']].to_csv(final_path, index=False)\n",
    "    print(f\"\\n✓ Complete: {successful} successful, {failed} failed, {skipped} skipped\")\n",
    "    print(f\"✓ Saved: {final_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "steam_games_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
